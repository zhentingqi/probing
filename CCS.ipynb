{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement CCS from scratch.\n",
    "This will deliberately be a simple (but less efficient) implementation to make everything as clear as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM, LlamaForCausalLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Let's just try IMDB for simplicity\n",
    "print(\"Loading data...\")\n",
    "# data = load_dataset(\"amazon_polarity\")[\"test\"]\n",
    "with open(\"/root/zhenting_a5000/probing/data/lfqa_umd_transformed.jsonl\", \"r\") as fin:\n",
    "    data = [json.loads(line) for line in fin]\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "{'content': 'Q: why cant america just ban guns\\nA: For the same reason you can’t ban free speech.  It is a constitutional freedom that was designed to be difficult dismantle.', 'label': 0}\n",
      "dict_keys(['content', 'label'])\n",
      "Q: why cant america just ban guns\n",
      "A: For the same reason you can’t ban free speech.  It is a constitutional freedom that was designed to be difficult dismantle.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# print(data)\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "print(data[0].keys())\n",
    "print(data[0]['content'])\n",
    "print(data[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fb22d7bdf841739831037925037bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here are a few different model options you can play around with:\n",
    "# model_name = \"deberta\"\n",
    "# model_name = \"gpt-j\"\n",
    "# model_name = \"t5\"\n",
    "model_name = \"llama\"\n",
    "\n",
    "# if you want to cache the model weights somewhere, you can specify that here\n",
    "cache_dir = \"/root/autodl-fs/model_cache\"\n",
    "\n",
    "if model_name == \"deberta\":\n",
    "    model_type = \"encoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xxlarge\", cache_dir=cache_dir)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"microsoft/deberta-v2-xxlarge\", cache_dir=cache_dir)\n",
    "    model.cuda()\n",
    "elif model_name == \"gpt-j\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\", cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", cache_dir=cache_dir)\n",
    "    model.cuda()\n",
    "elif model_name == \"t5\":\n",
    "    model_type = \"encoder_decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-11b\", cache_dir=cache_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-11b\", cache_dir=cache_dir)\n",
    "    model.parallelize()  # T5 is big enough that we may need to run it on multiple GPUs\n",
    "elif model_name == \"llama\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-fs/llama/llama-2-7b-chat-to-hf/\")\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = LlamaForCausalLM.from_pretrained(\"/root/autodl-fs/llama/llama-2-7b-chat-to-hf/\").half().cuda().eval() #! add \"half()\" to fit in a smaller GPU\n",
    "else:\n",
    "    print(\"Not implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's write code for extracting hidden states given a model and text. \n",
    "How we do this exactly will depend on the type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_encoder_decoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    decoder_text_ids = tokenizer(\"\", return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, decoder_input_ids=decoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"encoder_hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_decoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some text, gets the hidden states (in a given layer, by default the last) on that input text\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize (adding the EOS token this time)\n",
    "    input_ids = tokenizer(input_text + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_hidden_states(model, tokenizer, input_text, layer=-1, model_type=\"encoder\"):\n",
    "    fn = {\"encoder\": get_encoder_hidden_states, \"encoder_decoder\": get_encoder_decoder_hidden_states,\n",
    "          \"decoder\": get_decoder_hidden_states}[model_type]\n",
    "\n",
    "    return fn(model, tokenizer, input_text, layer=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's write code for formatting data and for getting all the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_imdb(text, label):\n",
    "    \"\"\"\n",
    "    Given an imdb example (\"text\") and corresponding label (0 for negative, or 1 for positive), \n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "    \n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"The following movie review expresses a \" + [\"negative\", \"positive\"][label] + \" sentiment:\\n\" + text\n",
    "\n",
    "\n",
    "def format_customized(text, label):\n",
    "    return \"The answer to the following QA is given by \" + [\"human\", \"machine\"][label] + \":\\n\" + text\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, model_type, n=100):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    \n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop: sample n data items \n",
    "    for _ in tqdm(range(n)):\n",
    "        #! for simplicity, sample a random example until we find one that's a reasonable length\n",
    "        # (most examples should be a reasonable length, so this is just to make sure)\n",
    "        while True:\n",
    "            idx = np.random.randint(len(data))\n",
    "            text, true_label = data[idx][\"content\"], data[idx][\"label\"]\n",
    "            # the actual formatted input will be longer, so include a bit of a marign\n",
    "            if len(tokenizer(text)) < 400:  \n",
    "                break\n",
    "                \n",
    "        #! get hidden states\n",
    "        neg_hs = get_hidden_states(model, tokenizer, input_text=format_customized(text, 0), model_type=model_type)\n",
    "        pos_hs = get_hidden_states(model, tokenizer, input_text=format_customized(text, 1), model_type=model_type)\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "        all_gt_labels.append(true_label)\n",
    "\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "    all_gt_labels = np.stack(all_gt_labels)\n",
    "\n",
    "    return all_neg_hs, all_pos_hs, all_gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.59it/s]\n"
     ]
    }
   ],
   "source": [
    "neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, data, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's verify that the model's representations are good\n",
    "\n",
    "Before trying CCS, let's make sure there exists a direction that classifies examples as true vs false with high accuracy; if logistic regression accuracy is bad, there's no hope of CCS doing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "# let's create a simple 50/50 train split (the data is already randomized)\n",
    "n = len(y)  # number of samples\n",
    "neg_hs_train, neg_hs_test = neg_hs[:n//2], neg_hs[n//2:]\n",
    "pos_hs_train, pos_hs_test = pos_hs[:n//2], pos_hs[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]\n",
    "\n",
    "# for simplicity we can just take the difference between positive and negative hidden states\n",
    "# 这个相减之后的向量可以理解为“正转负所需的变化向量”，既从pos到neg的语义转变所需要的向量\n",
    "# 能训练一个好的线性分类器，就意味着，模型对于 转向真负语义(y=0)/没转向真负语义(y=1) 的变化向量 有线性边界，也就意味着模型“知道”一个语义本身是正是负\n",
    "# (concatenating also works fine)\n",
    "x_train = neg_hs_train - pos_hs_train\n",
    "x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(x_train, y_train)\n",
    "print(\"Logistic regression accuracy: {}\".format(lr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProbe(nn.Module):\n",
    "    #! 一个比LogisticRegression稍复杂一些的探针\n",
    "    def __init__(self, d, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d, hidden_size)\n",
    "        # self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        # self.linear = nn.Linear(d, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        # x = F.relu(self.linear2(x))\n",
    "        o = self.linear3(x)\n",
    "        # o = self.linear(x)\n",
    "        return torch.sigmoid(o)\n",
    "\n",
    "class CCS(object):\n",
    "    def __init__(self, \n",
    "                 x0, x1, \n",
    "                 nepochs=2000, ntries=10, lr=1e-4, batch_size=-1, \n",
    "                 verbose=False, device=\"cuda\", linear=True, weight_decay=0.01, var_normalize=False):\n",
    "        # data\n",
    "        self.var_normalize = var_normalize\n",
    "        self.x0 = self.normalize(x0)\n",
    "        self.x1 = self.normalize(x1)\n",
    "        self.d = self.x0.shape[-1]\n",
    "\n",
    "        # training\n",
    "        self.nepochs = nepochs\n",
    "        self.ntries = ntries\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # probe\n",
    "        self.linear = linear\n",
    "        self.initialize_probe()\n",
    "        self.best_probe = copy.deepcopy(self.probe)\n",
    "\n",
    "        \n",
    "    def initialize_probe(self):\n",
    "        if self.linear:\n",
    "            self.probe = nn.Sequential(nn.Linear(self.d, 1), nn.Sigmoid())\n",
    "        else:\n",
    "            self.probe = MLPProbe(self.d)\n",
    "        self.probe.to(self.device)    \n",
    "\n",
    "\n",
    "    def normalize(self, x):\n",
    "        \"\"\"\n",
    "        Mean-normalizes the data x (of shape (n, d))\n",
    "        If self.var_normalize, also divides by the standard deviation\n",
    "        \"\"\"\n",
    "        normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "        if self.var_normalize:\n",
    "            normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "        return normalized_x\n",
    "\n",
    "        \n",
    "    def get_tensor_data(self):\n",
    "        \"\"\"\n",
    "        Returns x0, x1 as appropriate tensors (rather than np arrays)\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.x0, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.x1, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        return x0, x1\n",
    "    \n",
    "\n",
    "    def get_loss(self, p0, p1):\n",
    "        \"\"\"\n",
    "        Returns the CCS loss for two probabilities each of shape (n,1) or (n,)\n",
    "        \"\"\"\n",
    "        informative_loss = (torch.min(p0, p1)**2).mean(0)\n",
    "        consistent_loss = ((p0 - (1-p1))**2).mean(0)\n",
    "        return informative_loss + consistent_loss\n",
    "\n",
    "\n",
    "    def get_acc(self, x0_test, x1_test, y_test):\n",
    "        \"\"\"\n",
    "        Computes accuracy for the current parameters on the given test inputs\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n",
    "        avg_confidence = 0.5*(p0 + (1-p1))\n",
    "        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n",
    "        acc = (predictions == y_test).mean()\n",
    "        acc = max(acc, 1 - acc)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Does a single training run of nepochs epochs\n",
    "        \"\"\"\n",
    "        x0, x1 = self.get_tensor_data()\n",
    "        permutation = torch.randperm(len(x0))\n",
    "        x0, x1 = x0[permutation], x1[permutation]\n",
    "        \n",
    "        # set up optimizer\n",
    "        optimizer = torch.optim.AdamW(self.probe.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        batch_size = len(x0) if self.batch_size == -1 else self.batch_size\n",
    "        nbatches = len(x0) // batch_size\n",
    "\n",
    "        # Start training (full batch)\n",
    "        for epoch in range(self.nepochs):\n",
    "            for j in range(nbatches):\n",
    "                x0_batch = x0[j*batch_size:(j+1)*batch_size]\n",
    "                x1_batch = x1[j*batch_size:(j+1)*batch_size]\n",
    "            \n",
    "                # probe\n",
    "                p0, p1 = self.probe(x0_batch), self.probe(x1_batch)\n",
    "\n",
    "                # get the corresponding loss\n",
    "                loss = self.get_loss(p0, p1)\n",
    "\n",
    "                # update the parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return loss.detach().cpu().item()\n",
    "    \n",
    "    def repeated_train(self):\n",
    "        best_loss = np.inf\n",
    "        loss_list= []\n",
    "        for train_num in trange(self.ntries):\n",
    "            self.initialize_probe()\n",
    "            loss = self.train()\n",
    "            loss_list.append(loss)\n",
    "            if loss < best_loss:\n",
    "                self.best_probe = copy.deepcopy(self.probe)\n",
    "                best_loss = loss\n",
    "        \n",
    "        plt.plot(list(range(len(loss_list))), loss_list)\n",
    "        plt.show()\n",
    "        plt.savefig(\"./out/loss.jpg\")\n",
    "        return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:33<00:00,  3.31s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCS accuracy: 0.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train CCS without any labels\n",
    "ccs = CCS(neg_hs_train, pos_hs_train)\n",
    "ccs.repeated_train()\n",
    "\n",
    "# Evaluate\n",
    "ccs_acc = ccs.get_acc(neg_hs_test, pos_hs_test, y_test)\n",
    "print(\"CCS accuracy: {}\".format(ccs_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b80286374679f2ad472c61c83fc267d31329b5dea8e2dcaccb727123767724c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
