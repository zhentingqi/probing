{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement CCS from scratch.\n",
    "This will deliberately be a simple (but less efficient) implementation to make everything as clear as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM, LlamaForCausalLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "setup_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Let's just try IMDB for simplicity\n",
    "print(\"Loading data...\")\n",
    "# data = load_dataset(\"amazon_polarity\")[\"test\"]\n",
    "with open(\"/root/zhenting_a5000/probing/data/lfqa_umd_transformed.jsonl\", \"r\") as fin:\n",
    "    data = [json.loads(line) for line in fin]\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "{'content': 'Q: why cant america just ban guns\\nA: For the same reason you can’t ban free speech.  It is a constitutional freedom that was designed to be difficult dismantle.', 'label': 0}\n",
      "dict_keys(['content', 'label'])\n",
      "Q: why cant america just ban guns\n",
      "A: For the same reason you can’t ban free speech.  It is a constitutional freedom that was designed to be difficult dismantle.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# print(data)\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "print(data[0].keys())\n",
    "print(data[0]['content'])\n",
    "print(data[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46545290da04318940912cd139a2ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here are a few different model options you can play around with:\n",
    "# model_name = \"deberta\"\n",
    "# model_name = \"gpt-j\"\n",
    "# model_name = \"t5\"\n",
    "model_name = \"llama\"\n",
    "\n",
    "# if you want to cache the model weights somewhere, you can specify that here\n",
    "cache_dir = \"/root/autodl-fs/model_cache\"\n",
    "\n",
    "if model_name == \"deberta\":\n",
    "    model_type = \"encoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xxlarge\", cache_dir=cache_dir)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"microsoft/deberta-v2-xxlarge\", cache_dir=cache_dir)\n",
    "    model.cuda()\n",
    "elif model_name == \"gpt-j\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\", cache_dir=cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", cache_dir=cache_dir)\n",
    "    model.cuda()\n",
    "elif model_name == \"t5\":\n",
    "    model_type = \"encoder_decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-11b\", cache_dir=cache_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-11b\", cache_dir=cache_dir)\n",
    "    model.parallelize()  # T5 is big enough that we may need to run it on multiple GPUs\n",
    "elif model_name == \"llama\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-fs/llama/llama-2-7b-chat-to-hf/\")\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = LlamaForCausalLM.from_pretrained(\"/root/autodl-fs/llama/llama-2-7b-chat-to-hf/\").half().cuda().eval() #! add \"half()\" to fit in a smaller GPU\n",
    "else:\n",
    "    print(\"Not implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's write code for extracting hidden states given a model and text. \n",
    "How we do this exactly will depend on the type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, truncation=True, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    \n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_encoder_decoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model and some text, gets the encoder hidden states (in a given layer, by default the last) \n",
    "    on that input text (where the full text is given to the encoder).\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    encoder_text_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    decoder_text_ids = tokenizer(\"\", return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_text_ids, decoder_input_ids=decoder_text_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the appropriate hidden states\n",
    "    hs_tuple = output[\"encoder_hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_decoder_hidden_states(model, tokenizer, input_text, layer=-1):\n",
    "    \"\"\"\n",
    "    Given a decoder model and some text, gets the hidden states (in a given layer, by default the last) on that input text\n",
    "\n",
    "    Returns a numpy array of shape (hidden_dim,)\n",
    "    \"\"\"\n",
    "    # tokenize (adding the EOS token this time)\n",
    "    input_ids = tokenizer(input_text + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    # get the last layer, last token hidden states\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_hidden_states(model, tokenizer, input_text, layer=-1, model_type=\"encoder\"):\n",
    "    fn = {\"encoder\": get_encoder_hidden_states, \"encoder_decoder\": get_encoder_decoder_hidden_states,\n",
    "          \"decoder\": get_decoder_hidden_states}[model_type]\n",
    "\n",
    "    return fn(model, tokenizer, input_text, layer=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's write code for formatting data and for getting all the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_imdb(text, label):\n",
    "    \"\"\"\n",
    "    Given an imdb example (\"text\") and corresponding label (0 for negative, or 1 for positive), \n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "    \n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"The following movie review expresses a \" + [\"negative\", \"positive\"][label] + \" sentiment:\\n\" + text\n",
    "\n",
    "\n",
    "def format_customized(text, label):\n",
    "    return \"The answer to the following QA is given by \" + [\"human\", \"machine\"][label] + \":\\n\" + text\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, model_type, n=1024):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples.\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    \n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    print(f\"Size of data: {len(data)}\")\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "\n",
    "    # loop: sample n data items \n",
    "    for _ in tqdm(range(n)):\n",
    "        #! for simplicity, sample a random example until we find one that's a reasonable length\n",
    "        # (most examples should be a reasonable length, so this is just to make sure)\n",
    "        while True:\n",
    "            idx = np.random.randint(len(data))\n",
    "            text, true_label = data[idx][\"content\"], data[idx][\"label\"]\n",
    "            # the actual formatted input will be longer, so include a bit of a marign\n",
    "            if len(tokenizer(text)) < 400:  \n",
    "                break\n",
    "                \n",
    "        #! get hidden states\n",
    "        neg_hs = get_hidden_states(model, tokenizer, input_text=format_customized(text, 0), model_type=model_type)\n",
    "        pos_hs = get_hidden_states(model, tokenizer, input_text=format_customized(text, 1), model_type=model_type)\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "        all_gt_labels.append(true_label)\n",
    "\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "    all_gt_labels = np.stack(all_gt_labels)\n",
    "\n",
    "    return all_neg_hs, all_pos_hs, all_gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [03:03<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, data, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's verify that the model's representations are good\n",
    "\n",
    "Before trying CCS, let's make sure there exists a direction that classifies examples as true vs false with high accuracy; if logistic regression accuracy is bad, there's no hope of CCS doing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.99609375, precision: 0.9925373134328358, recall: 1.0, f1: 0.9962546816479401\n"
     ]
    }
   ],
   "source": [
    "# let's create a simple 50/50 train split (the data is already randomized)\n",
    "n = len(y)  # number of samples\n",
    "neg_hs_train, neg_hs_test = neg_hs[:n//2], neg_hs[n//2:]\n",
    "pos_hs_train, pos_hs_test = pos_hs[:n//2], pos_hs[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]\n",
    "\n",
    "# for simplicity we can just take the difference between positive and negative hidden states\n",
    "# 这个相减之后的向量可以理解为“正转负所需的变化向量”，既从pos到neg的语义转变所需要的向量\n",
    "# 能训练一个好的线性分类器，就意味着，模型对于 转向真负语义(y=0)/没转向真负语义(y=1) 的变化向量 有线性边界，也就意味着模型“知道”一个语义本身是正是负\n",
    "# (concatenating also works fine)\n",
    "x_train = neg_hs_train - pos_hs_train\n",
    "x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(x_train, y_train)\n",
    "predictions = lr.predict(x_test)\n",
    "# print(predictions)\n",
    "# print(y_test)\n",
    "accuracy, precision, recall, f1 = (f(y_test, predictions) for f in [accuracy_score, precision_score, recall_score, f1_score])\n",
    "print(f\"accuracy: {accuracy}, precision: {precision}, recall: {recall}, f1: {f1}\")\n",
    "# print(\"Logistic regression accuracy: {}\".format(lr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProbe(nn.Module):\n",
    "    #! 一个比LogisticRegression稍复杂一些的探针\n",
    "    def __init__(self, d, hidden_size=8192, more_layers=6):\n",
    "        super().__init__()\n",
    "        self.linear_start = nn.Linear(d, hidden_size)\n",
    "        self.act_start = nn.ReLU()\n",
    "        if more_layers > 0:\n",
    "            self.more_layers = nn.Sequential([nn.Linear, nn.ReLU] * more_layers)\n",
    "        self.linear_end = nn.Linear(hidden_size, 1)\n",
    "        self.act_end = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_start(x)\n",
    "        x = self.act_start(x)\n",
    "        if self.more_layers:\n",
    "            x = self.more_layers(x)\n",
    "        x = self.linear_end(x)\n",
    "        x = self.act_end(x)\n",
    "        return x\n",
    "\n",
    "class CCS(object):\n",
    "    def __init__(self, \n",
    "                 x0, x1, \n",
    "                 nepochs=5000, ntries=1, lr=1e-4, batch_size=-1, \n",
    "                 verbose=False, device=\"cuda\", linear=True, weight_decay=0.01, var_normalize=False):\n",
    "        # data\n",
    "        self.var_normalize = var_normalize\n",
    "        self.x0 = self.normalize(x0)\n",
    "        self.x1 = self.normalize(x1)\n",
    "        self.d = self.x0.shape[-1]\n",
    "\n",
    "        # training\n",
    "        self.nepochs = nepochs\n",
    "        self.ntries = ntries\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # probe\n",
    "        self.linear = linear\n",
    "        self.initialize_probe()\n",
    "        self.best_probe = copy.deepcopy(self.probe)\n",
    "\n",
    "        \n",
    "    def initialize_probe(self):\n",
    "        if self.linear:\n",
    "            self.probe = nn.Sequential(nn.Linear(self.d, 1), nn.Sigmoid())\n",
    "        else:\n",
    "            self.probe = MLPProbe(self.d)\n",
    "        self.probe.to(self.device)    \n",
    "\n",
    "\n",
    "    def normalize(self, x):\n",
    "        \"\"\"\n",
    "        Mean-normalizes the data x (of shape (n, d))\n",
    "        If self.var_normalize, also divides by the standard deviation\n",
    "        \"\"\"\n",
    "        normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "        if self.var_normalize:\n",
    "            normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "        return normalized_x\n",
    "\n",
    "        \n",
    "    def get_tensor_data(self):\n",
    "        \"\"\"\n",
    "        Returns x0, x1 as appropriate tensors (rather than np arrays)\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.x0, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.x1, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        return x0, x1\n",
    "    \n",
    "\n",
    "    def get_loss(self, p0, p1):\n",
    "        \"\"\"\n",
    "        Returns the CCS loss for two probabilities each of shape (n,1) or (n,)\n",
    "        \"\"\"\n",
    "        informative_loss = (torch.min(p0, p1)**2).mean(0)\n",
    "        consistent_loss = ((p0 - (1-p1))**2).mean(0)\n",
    "        return informative_loss + consistent_loss\n",
    "\n",
    "\n",
    "    def get_acc(self, x0_test, x1_test, y_test):\n",
    "        \"\"\"\n",
    "        Computes accuracy for the current parameters on the given test inputs\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n",
    "        avg_confidence = 0.5*(p0 + (1-p1))\n",
    "        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n",
    "        acc = (predictions == y_test).mean()\n",
    "        acc = max(acc, 1 - acc)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "    \n",
    "    def get_acc_precision_recall_f1(self, x0_test, x1_test, y_test):\n",
    "        x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n",
    "        avg_confidence = 0.5*(p0 + (1-p1))\n",
    "        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n",
    "\n",
    "        accuracy, precision, recall, f1 = (f(y_test, predictions) for f in [accuracy_score, precision_score, recall_score, f1_score])\n",
    "        print(f\"accuracy: {accuracy}, precision: {precision}, recall: {recall}, f1: {f1}\")\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Does a single training run of nepochs epochs\n",
    "        \"\"\"\n",
    "        x0, x1 = self.get_tensor_data()\n",
    "        permutation = torch.randperm(len(x0))\n",
    "        x0, x1 = x0[permutation], x1[permutation]\n",
    "        \n",
    "        # set up optimizer\n",
    "        optimizer = torch.optim.AdamW(self.probe.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        batch_size = len(x0) if self.batch_size == -1 else self.batch_size\n",
    "        nbatches = len(x0) // batch_size\n",
    "\n",
    "        # Start training (full batch)\n",
    "        loss_list= []\n",
    "        for epoch in trange(self.nepochs):\n",
    "            for j in range(nbatches):\n",
    "                x0_batch = x0[j*batch_size:(j+1)*batch_size]\n",
    "                x1_batch = x1[j*batch_size:(j+1)*batch_size]\n",
    "            \n",
    "                # probe\n",
    "                p0, p1 = self.probe(x0_batch), self.probe(x1_batch)\n",
    "\n",
    "                # get the corresponding loss\n",
    "                loss = self.get_loss(p0, p1)\n",
    "                loss_list.append(loss.detach().cpu().item())\n",
    "\n",
    "                # update the parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        plt.plot(list(range(len(loss_list))), loss_list)\n",
    "        plt.show()\n",
    "        \n",
    "        return loss.detach().cpu().item()\n",
    "    \n",
    "    def repeated_train(self):\n",
    "        best_loss = np.inf\n",
    "\n",
    "        for train_num in range(self.ntries):\n",
    "            self.initialize_probe()\n",
    "            loss = self.train()\n",
    "            if loss < best_loss:\n",
    "                self.best_probe = copy.deepcopy(self.probe)\n",
    "                best_loss = loss\n",
    "\n",
    "        return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 124/5000 [00:00<00:07, 623.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:09<00:00, 525.77it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.623046875, precision: 0.6308243727598566, recall: 0.6616541353383458, f1: 0.6458715596330274\n"
     ]
    }
   ],
   "source": [
    "# Train CCS without any labels\n",
    "ccs = CCS(neg_hs_train, pos_hs_train)\n",
    "ccs.repeated_train()\n",
    "\n",
    "# Evaluate\n",
    "ccs.get_acc_precision_recall_f1(neg_hs_test, pos_hs_test, y_test)\n",
    "# print(\"CCS accuracy: {}\".format(ccs_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b80286374679f2ad472c61c83fc267d31329b5dea8e2dcaccb727123767724c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
